{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EfficientSeg.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","from torch import nn\n","device = torch.device(\"cuda:0\")"],"metadata":{"id":"O5PxtakFKPoW","executionInfo":{"status":"ok","timestamp":1651102821924,"user_tz":-60,"elapsed":3070,"user":{"displayName":"Amany Fetah","userId":"03251817888725515836"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def _make_divisible(v, divisor, min_value=None):\n","    \"\"\"\n","    This function is taken from the original tf repo.\n","    It ensures that all layers have a channel number that is divisible by 8\n","    It can be seen here:\n","    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n","    :param v:\n","    :param divisor:\n","    :param min_value:\n","    :return:\n","    \"\"\"\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v"],"metadata":{"id":"wQhIREhoKfIL","executionInfo":{"status":"ok","timestamp":1651102823563,"user_tz":-60,"elapsed":300,"user":{"displayName":"Amany Fetah","userId":"03251817888725515836"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"MwcuPnSNKA70","executionInfo":{"status":"ok","timestamp":1651102826459,"user_tz":-60,"elapsed":355,"user":{"displayName":"Amany Fetah","userId":"03251817888725515836"}}},"outputs":[],"source":["class h_sigmoid(nn.Module):\n","    def __init__(self, inplace=True):\n","        super(h_sigmoid, self).__init__()\n","        self.relu = nn.ReLU6(inplace=inplace)\n","\n","    def forward(self, x):\n","        return self.relu(x + 3) / 6\n","\n","\n","class h_swish(nn.Module):\n","    def __init__(self, inplace=True):\n","        super(h_swish, self).__init__()\n","        self.sigmoid = h_sigmoid(inplace=inplace)\n","\n","    def forward(self, x):\n","        return x * self.sigmoid(x)\n","\n","\n","class SELayer(nn.Module):\n","    def __init__(self, channel, reduction=4):\n","        super(SELayer, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n","                nn.ReLU(inplace=True),\n","                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n","                h_sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y\n","\n","\n","def conv_3x3_bn(inp, oup, stride):\n","    return nn.Sequential(\n","        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n","        nn.BatchNorm2d(oup),\n","        h_swish()\n","    )\n","\n","\n","def conv_1x1_bn(inp, oup):\n","    return nn.Sequential(\n","        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n","        nn.BatchNorm2d(oup),\n","        h_swish()\n","    )\n","\n","\n","class InvertedResidual(nn.Module):\n","    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n","        super(InvertedResidual, self).__init__()\n","        assert stride in [1, 2]\n","\n","        self.identity = stride == 1 and inp == oup\n","\n","        if inp == hidden_dim:\n","            self.conv = nn.Sequential(\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                h_swish() if use_hs else nn.ReLU(inplace=True),\n","                # Squeeze-and-Excite\n","                SELayer(hidden_dim) if use_se else nn.Identity(),\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","        else:\n","            self.conv = nn.Sequential(\n","                # pw\n","                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                h_swish() if use_hs else nn.ReLU(inplace=True),\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                # Squeeze-and-Excite\n","                SELayer(hidden_dim) if use_se else nn.Identity(),\n","                h_swish() if use_hs else nn.ReLU(inplace=True),\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","\n","    def forward(self, x):\n","        if self.identity:\n","            return x + self.conv(x)\n","        else:\n","            return self.conv(x)\n","\n"]},{"cell_type":"code","source":["class EfficientSeg(nn.Module):\n","    def __init__(self, enc_config, dec_config, num_classes, width_coeff):\n","        super(EfficientSeg, self).__init__()\n","\n","        block = InvertedResidual\n","\n","        input_channel = _make_divisible(16 * width_coeff, 8)\n","\n","        self.enc_layers = nn.ModuleList()\n","        self.enc_layers.append( conv_3x3_bn(3, input_channel, 2) )\n","\n","        self.dec_layers = nn.ModuleList()\n","        self.dec_layers.append( conv_3x3_bn(input_channel, num_classes, 1) )\n","\n","        self.down_map = []\n","        last_s = None\n","\n","        for idx, (k, t, c, use_se, use_hs, s) in enumerate(enc_config):\n","            output_channel = _make_divisible(c * width_coeff, 8)\n","            exp_size = _make_divisible(input_channel * t, 8)\n","            self.enc_layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n","\n","            if idx < len(enc_config) - 1 and enc_config[idx+1][-1] == 2:\n","                self.dec_layers.insert(0,block(output_channel*2, exp_size, input_channel, k, 1, use_se, use_hs))\n","            else:\n","                self.dec_layers.insert(0,block(output_channel, exp_size, input_channel, k, 1, use_se, use_hs))\n","\n","\n","            input_channel = output_channel\n","            last_s = s\n","\n","            if( s == 1 ): self.down_map.append(0)\n","            else: self.down_map.append(1)\n","\n","        self.conv = conv_1x1_bn(input_channel, exp_size)\n","        self.conv2 = conv_1x1_bn(exp_size, input_channel)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n","\n","\n","\n","\n","    def forward(self, x):\n","\n","        concatputs = []\n","\n","        for idx, layer in enumerate(self.enc_layers):\n","            if idx > 0 and self.down_map[idx-1] == 1:\n","                concatputs.append(x)\n","            x = layer(x)\n","        \n","        x = self.conv(x)\n","        x = self.conv2(x)\n","\n","        conc_put_idx = len(concatputs) - 1\n","\n","        for idx, layer in enumerate(self.dec_layers):\n","            \n","            if idx < len(self.dec_layers) - 1:\n","                if self.down_map[-1-idx] == 1:\n","                    x = self.upsample(x)\n","                if self.down_map[-idx] == 1:\n","                    x = torch.cat((x, concatputs[conc_put_idx]), dim=1)\n","                    conc_put_idx -= 1\n","\n","            if idx == len(self.dec_layers) - 1:\n","                x = self.upsample(x)\n","\n","            x = layer(x)\n","\n","\n","\n","        return x\n","\n","\n","\n","enc_config = [\n","    # k, t, c, SE, HS, s \n","    [3,   1,  16, 0, 0, 1],\n","    [3,   4,  24, 0, 0, 2],\n","    [3,   3,  24, 0, 0, 1],\n","    [5,   3,  40, 1, 0, 2],\n","    [5,   3,  40, 1, 0, 1],\n","    [5,   3,  40, 1, 0, 1],\n","    [3,   6,  80, 0, 1, 2],\n","    [3, 2.5,  80, 0, 1, 1],\n","    [3, 2.3,  80, 0, 1, 1],\n","    [3, 2.3,  80, 0, 1, 1],\n","    [3,   6, 112, 1, 1, 1],\n","    [3,   6, 112, 1, 1, 1],\n","    [5,   6, 160, 1, 1, 2],\n","    [5,   6, 160, 1, 1, 1],\n","    [5,   6, 160, 1, 1, 1]\n","]\n","\n","#inp = torch.rand(1,3,256,256).to( torch.device(\"cuda:0\") )\n","#out = model(inp)\n"],"metadata":{"id":"OW8D1I7BKnQT","executionInfo":{"status":"ok","timestamp":1651102830745,"user_tz":-60,"elapsed":490,"user":{"displayName":"Amany Fetah","userId":"03251817888725515836"}}},"execution_count":4,"outputs":[]}]}