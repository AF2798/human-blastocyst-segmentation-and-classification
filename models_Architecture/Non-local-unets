{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Non-local-unets","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.keras.layers"],"metadata":{"id":"y8bevY_aJNb0","executionInfo":{"status":"ok","timestamp":1651674320628,"user_tz":-60,"elapsed":410,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def Pool3d(inputs, kernel_size, strides):\n","\t\"\"\"Performs 3D max pooling.\"\"\"\n","\n","\treturn tf.compat.v1.layers.max_pooling3d(\n","\t\t\tinputs=inputs,\n","\t\t\tpool_size=kernel_size,\n","\t\t\tstrides=strides,\n","\t\t\tpadding='same')\n","\n","\n","def Deconv3D(inputs, filters, kernel_size, strides, use_bias=False):\n","\t\"\"\"Performs 3D deconvolution without bias and activation function.\"\"\"\n","\n","\treturn tf.compat.v1.layers.conv3d_transpose(\n","\t\t\tinputs=inputs,\n","\t\t\tfilters=filters,\n","\t\t\tkernel_size=kernel_size,\n","\t\t\tstrides=strides,\n","\t\t\tpadding='same',\n","\t\t\tuse_bias=use_bias,\n","\t\t\tkernel_initializer=tf.truncated_normal_initializer())\n","\n","\n","def Conv3D(inputs, filters, kernel_size, strides, use_bias=False):\n","\t\"\"\"Performs 3D convolution without bias and activation function.\"\"\"\n","\n","\treturn tf.compat.v1.layers.conv3d(\n","\t\t\tinputs=inputs,\n","\t\t\tfilters=filters,\n","\t\t\tkernel_size=kernel_size,\n","\t\t\tstrides=strides,\n","\t\t\tpadding='same',\n","\t\t\tuse_bias=use_bias)\n","\n","\n","def Dilated_Conv3D(inputs, filters, kernel_size, dilation_rate, use_bias=False):\n","\t\"\"\"Performs 3D dilated convolution without bias and activation function.\"\"\"\n","\n","\treturn tf.compat.v1.layers.conv3d(\n","\t\t\tinputs=inputs,\n","\t\t\tfilters=filters,\n","\t\t\tkernel_size=kernel_size,\n","\t\t\tstrides=1,\n","\t\t\tdilation_rate=dilation_rate,\n","\t\t\tpadding='same',\n","\t\t\tuse_bias=use_bias,\n","\t\t\tkernel_initializer=tf.truncated_normal_initializer())\n","\n","\n","def BN_ReLU(inputs, training):\n","\t\"\"\"Performs a batch normalization followed by a ReLU6.\"\"\"\n","\n","\t# We set fused=True for a significant performance boost. See\n","\t# https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n","\tinputs = tf.compat.v1.layers.batch_normalization(\n","\t\t\t\tinputs=inputs,\n","\t\t\t\taxis=-1,\n","\t\t\t\tmomentum=0.997,\n","\t\t\t\tepsilon=1e-5,\n","\t\t\t\tcenter=True,\n","\t\t\t\tscale=True,\n","\t\t\t\ttraining=training, \n","\t\t\t\tfused=True)\n","\n","\treturn tf.nn.relu6(inputs)"],"metadata":{"id":"a47OpB-rI-7V","executionInfo":{"status":"ok","timestamp":1651674321122,"user_tz":-60,"elapsed":4,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["def compute_qkv_3d(inputs, total_key_filters, total_value_filters, layer_type):\n","\t\"\"\"Computes query, key and value.\n","\tArgs:\n","\t\tinputs: a Tensor with shape [batch, d, h, w, channels]\n","\t\ttotal_key_filters: an integer\n","\t\ttotal_value_filters: and integer\n","\t\tlayer_type: String, type of this layer -- SAME, DOWN, UP\n","\t\n","\tReturns:\n","\t\tq: [batch, _d, _h, _w, total_key_filters] tensor\n","\t\tk: [batch, h, w, total_key_filters] tensor\n","\t\tv: [batch, h, w, total_value_filters] tensor\n","\t\"\"\"\n","\n","\t# linear transformation for q\n","\tif layer_type == 'SAME':\n","\t\tq = Conv3D(inputs, total_key_filters, 1, 1, use_bias=True)\n","\telif layer_type == 'DOWN':\n","\t\tq = Conv3D(inputs, total_key_filters, 3, 2, use_bias=True)\n","\telif layer_type == 'UP':\n","\t\tq = Deconv3D(inputs, total_key_filters, 3, 2, use_bias=True)\n","\n","\t# linear transformation for k\n","\tk = Conv3D(inputs, total_key_filters, 1, 1, use_bias=True)\n","\n","\t# linear transformation for k\n","\tv = Conv3D(inputs, total_value_filters, 1, 1, use_bias=True)\n","\n","\treturn q, k, v\n","\n","\n","def split_heads_3d(x, num_heads):\n","\t\"\"\"Split channels (last dimension) into multiple heads (becomes dimension 1).\n","\t\n","\tArgs:\n","\t\tx: a Tensor with shape [batch, d, h, w, channels]\n","\t\tnum_heads: an integer\n","\t\n","\tReturns:\n","\t\ta Tensor with shape [batch, num_heads, d, h, w, channels / num_heads]\n","\t\"\"\"\n","\n","\treturn tf.transpose(split_last_dimension(x, num_heads), [0, 4, 1, 2, 3, 5])\n","\n","\n","def split_last_dimension(x, n):\n","\t\"\"\"Reshape x so that the last dimension becomes two dimensions.\n","\tThe first of these two dimensions is n.\n","\tArgs:\n","\t\tx: a Tensor with shape [..., m]\n","\t\tn: an integer.\n","\tReturns:\n","\t\ta Tensor with shape [..., n, m/n]\n","\t\"\"\"\n","\n","\told_shape = x.get_shape().dims\n","\tlast = old_shape[-1]\n","\tnew_shape = old_shape[:-1] + [n] + [last // n if last else None]\n","\t\n","\tret = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [n, -1]], 0))\n","\tret.set_shape(new_shape)\n","\t\n","\treturn ret\n","\n","\n","def global_attention_3d(q, k, v, training, name=None):\n","\t\"\"\"global self-attention.\n","\tArgs:\n","\t\tq: a Tensor with shape [batch, heads, _d, _h, _w, channels_k]\n","\t\tk: a Tensor with shape [batch, heads, d, h, w, channels_k]\n","\t\tv: a Tensor with shape [batch, heads, d, h, w, channels_v]\n","\t\tname: an optional string\n","\tReturns:\n","\t\ta Tensor of shape [batch, heads, _d, _h, _w, channels_v]\n","\t\"\"\"\n","\twith tf.variable_scope(\n","\t\t\tname,\n","\t\t\tdefault_name=\"global_attention_3d\",\n","\t\t\tvalues=[q, k, v]):\n","\n","\t\tnew_shape = tf.concat([tf.shape(q)[0:-1], [v.shape[-1].value]], 0)\n","\n","\t\t# flatten q,k,v\n","\t\tq_new = flatten_3d(q)\n","\t\tk_new = flatten_3d(k)\n","\t\tv_new = flatten_3d(v)\n","\n","\t\t# attention\n","\t\toutput = dot_product_attention(q_new, k_new, v_new, bias=None,\n","\t\t\t\t\ttraining=training, dropout_rate=0.5, name=\"global_3d\")\n","\n","\t\t# putting the representations back in the right place\n","\t\toutput = scatter_3d(output, new_shape)\n","\n","\t\treturn output\n","\n","\n","def reshape_range(tensor, i, j, shape):\n","\t\"\"\"Reshapes a tensor between dimensions i and j.\"\"\"\n","\n","\ttarget_shape = tf.concat(\n","\t\t\t[tf.shape(tensor)[:i], shape, tf.shape(tensor)[j:]],\n","\t\t\taxis=0)\n","\n","\treturn tf.reshape(tensor, target_shape)\n","\n","\n","def flatten_3d(x):\n","\t\"\"\"flatten x.\"\"\"\n","\n","\tx_shape = tf.shape(x)\n","\t# [batch, heads, length, channels], length = d*h*w\n","\tx = reshape_range(x, 2, 5, [tf.reduce_prod(x_shape[2:5])])\n","\n","\treturn x\n","\n","\n","def scatter_3d(x, shape):\n","\t\"\"\"scatter x.\"\"\"\n","\n","\tx = tf.reshape(x, shape)\n","\n","\treturn x\n","\n","\n","def dot_product_attention(q, k, v, bias, training, dropout_rate=0.0, name=None):\n","\t\"\"\"Dot-product attention.\n","\tArgs:\n","\t\tq: a Tensor with shape [batch, heads, length_q, channels_k]\n","\t\tk: a Tensor with shape [batch, heads, length_kv, channels_k]\n","\t\tv: a Tensor with shape [batch, heads, length_kv, channels_v]\n","\t\tbias: bias Tensor\n","\t\tdropout_rate: a floating point number\n","\t\tname: an optional string\n","\tReturns:\n","\t\tA Tensor with shape [batch, heads, length_q, channels_v]\n","\t\"\"\"\n","\n","\twith tf.variable_scope(\n","\t\t\tname,\n","\t\t\tdefault_name=\"dot_product_attention\",\n","\t\t\tvalues=[q, k, v]):\n","\n","\t\t# [batch, num_heads, length_q, length_kv]\n","\t\tlogits = tf.matmul(q, k, transpose_b=True)\n","\n","\t\tif bias is not None:\n","\t\t\tlogits += bias\n","\n","\t\tweights = tf.nn.softmax(logits, name=\"attention_weights\")\n","\n","\t\t# dropping out the attention links for each of the heads\n","\t\tweights = tf.compat.v1.layers.dropout(weights, dropout_rate, training)\n","\n","\t\treturn tf.matmul(weights, v)\n","\n","\n","def combine_heads_3d(x):\n","\t\"\"\"Inverse of split_heads_3d.\n","\tArgs:\n","\t\tx: a Tensor with shape [batch, num_heads, d, h, w, channels / num_heads]\n","\tReturns:\n","\t\ta Tensor with shape [batch, d, h, w, channels]\n","\t\"\"\"\n","\n","\treturn combine_last_two_dimensions(tf.transpose(x, [0, 2, 3, 4, 1, 5]))\n","\n","\n","def combine_last_two_dimensions(x):\n","\t\"\"\"Reshape x so that the last two dimension become one.\n","\tArgs:\n","\t\tx: a Tensor with shape [..., a, b]\n","\tReturns:\n","\t\ta Tensor with shape [..., a*b]\n","\t\"\"\"\n","\n","\told_shape = x.get_shape().dims\n","\ta, b = old_shape[-2:]\n","\tnew_shape = old_shape[:-2] + [a * b if a and b else None]\n","\n","\tret = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\n","\tret.set_shape(new_shape)\n","\n","\treturn ret"],"metadata":{"id":"bYTWafIZQxgi","executionInfo":{"status":"ok","timestamp":1651674321122,"user_tz":-60,"elapsed":3,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def multihead_attention_3d(inputs, total_key_filters, total_value_filters,\n","\t\t\t\t\t\t\toutput_filters, num_heads, training, layer_type='SAME',\n","\t\t\t\t\t\t\tname=None):\n","\t\"\"\"3d Multihead scaled-dot-product attention with input/output transformations.\n","\t\n","\tArgs:\n","\t\tinputs: a Tensor with shape [batch, d, h, w, channels]\n","\t\ttotal_key_filters: an integer. Note that queries have the same number \n","\t\t\tof channels as keys\n","\t\ttotal_value_filters: an integer\n","\t\toutput_depth: an integer\n","\t\tnum_heads: an integer dividing total_key_filters and total_value_filters\n","\t\tlayer_type: a string, type of this layer -- SAME, DOWN, UP\n","\t\tname: an optional string\n","\tReturns:\n","\t\tA Tensor of shape [batch, _d, _h, _w, output_filters]\n","\t\n","\tRaises:\n","\t\tValueError: if the total_key_filters or total_value_filters are not divisible\n","\t\t\tby the number of attention heads.\n","\t\"\"\"\n","\n","\tif total_key_filters % num_heads != 0:\n","\t\traise ValueError(\"Key depth (%d) must be divisible by the number of \"\n","\t\t\t\t\t\t\"attention heads (%d).\" % (total_key_filters, num_heads))\n","\tif total_value_filters % num_heads != 0:\n","\t\traise ValueError(\"Value depth (%d) must be divisible by the number of \"\n","\t\t\t\t\t\t\"attention heads (%d).\" % (total_value_filters, num_heads))\n","\tif layer_type not in ['SAME', 'DOWN', 'UP']:\n","\t\traise ValueError(\"Layer type (%s) must be one of SAME, \"\n","\t\t\t\t\t\t\"DOWN, UP.\" % (layer_type))\n","\n","\twith tf.variable_scope(\n","\t\t\tname,\n","\t\t\tdefault_name=\"multihead_attention_3d\",\n","\t\t\tvalues=[inputs]):\n","\n","\t\t# produce q, k, v\n","\t\tq, k, v = compute_qkv_3d(inputs, total_key_filters,\n","\t\t\t\t\t\ttotal_value_filters, layer_type)\n","\n","\t\t# after splitting, shape is [batch, heads, d, h, w, channels / heads]\n","\t\tq = split_heads_3d(q, num_heads)\n","\t\tk = split_heads_3d(k, num_heads)\n","\t\tv = split_heads_3d(v, num_heads)\n","\n","\t\t# normalize\n","\t\tkey_filters_per_head = total_key_filters // num_heads\n","\t\tq *= key_filters_per_head**-0.5\n","\n","\t\t# attention\n","\t\tx = global_attention_3d(q, k, v, training)\n","\t\t\n","\t\tx = combine_heads_3d(x)\n","\t\tx = Conv3D(x, output_filters, 1, 1, use_bias=True)\n","\n","\t\treturn x"],"metadata":{"id":"mBIxYzBSQbGx","executionInfo":{"status":"ok","timestamp":1651674321123,"user_tz":-60,"elapsed":4,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{"id":"AUGlCRgSaCoQ","executionInfo":{"status":"ok","timestamp":1651674960622,"user_tz":-60,"elapsed":665,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"outputs":[],"source":["\n","\n","\n","\n","\n","\"\"\"This script defines the network.\n","\"\"\"\n","\n","\n","num_classes = 4\n","num_filters = 32\n","block_sizes = [1] * 3\n","block_strides = [1] + [2] * (3 - 1)\n","\n","\n","def __call__( inputs, training):\n","\t\t\"\"\"Add operations to classify a batch of input images.\n","\t\tArgs:\n","\t\t\tinputs: A Tensor representing a batch of input images.\n","\t\t\ttraining: A boolean. Set to True to add operations required only when\n","\t\t\t\ttraining the classifier.\n","\t\tReturns:\n","\t\t\tA logits Tensor with shape [<batch_size>, num_classes].\n","\t\t\"\"\"\n","\n","\t\treturn _build_network(inputs, training)\n","\n","\n","\t################################################################################\n","\t# Composite blocks building the network\n","\t################################################################################\n","def _build_network( inputs, training):\n","\t\t\"\"\"Build the network.\n","\t\t\"\"\"\n","\n","\t\tinputs = Conv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=32,\n","\t\t\t\t\tkernel_size=3,\n","\t\t\t\t\tstrides=1)\n","\t\tinputs = tf.identity(inputs, 'initial_conv')\n","\n","\t\tskip_inputs = []\n","\t\tfor i, num_blocks in enumerate(block_sizes):\n","\t\t\t# print(i, num_blocks)\n","\t\t\tnum_filters = num_filters * (2**i)\n","\t\t\tinputs = _encoding_block_layer(\n","\t\t\t\t\t\tinputs=inputs, filters=num_filters,\n","\t\t\t\t\t\tblock_fn=_residual_block, blocks=num_blocks,\n","\t\t\t\t\t\tstrides=block_strides[i], training=training,\n","\t\t\t\t\t\tname='encode_block_layer{}'.format(i+1))\n","\t\t\tskip_inputs.append(inputs)\n","\t\t\t# print(inputs.shape)\n","\t\t# print(len(skip_inputs))\n","\t\t\n","\t\tinputs = BN_ReLU(inputs, training)\n","\t\tnum_filters = num_filters * (2**(len(block_sizes)-1))\n","\t\t# print(num_filters)\n","\t\tinputs = multihead_attention_3d(\n","\t\t\t\t\tinputs, num_filters, num_filters, num_filters, 2, training, layer_type='SAME')\n","\t\tinputs += skip_inputs[-1]\n","\n","\t\tfor i, num_blocks in reversed(list(enumerate(block_sizes[1:]))):\n","\t\t\t# print(i, num_blocks)\n","\t\t\tnum_filters = num_filters * (2**i)\n","\t\t\tif i == len(block_sizes) - 2:\n","\t\t\t\tinputs = _att_decoding_block_layer(\n","\t\t\t\t\t\tinputs=inputs, skip_inputs=skip_inputs[i],\n","\t\t\t\t\t\tfilters=num_filters, block_fn=_residual_block,\n","\t\t\t\t\t\tblocks=1, strides=block_strides[i+1],\n","\t\t\t\t\t\ttraining=training,\n","\t\t\t\t\t\tname='decode_block_layer{}'.format(len(block_sizes)-i-1))\n","\t\t\telse:\n","\t\t\t\tinputs = _decoding_block_layer(\n","\t\t\t\t\t\tinputs=inputs, skip_inputs=skip_inputs[i],\n","\t\t\t\t\t\tfilters=num_filters, block_fn=_residual_block,\n","\t\t\t\t\t\tblocks=1, strides=block_strides[i+1],\n","\t\t\t\t\t\ttraining=training,\n","\t\t\t\t\t\tname='decode_block_layer{}'.format(len(block_sizes)-i-1))\n","\t\t\t# print(inputs.shape)\n","\n","\t\tinputs = _output_block_layer(inputs=inputs, training=training)\n","\t\t# print(inputs.shape)\n","\n","\t\treturn inputs\n","\n","\n","\t################################################################################\n","\t# Composite blocks building the network\n","\t################################################################################\n","def _output_block_layer( inputs, training):\n","\n","\t\tinputs = BN_ReLU(inputs, training)\n","\n","\t\tinputs = tf.layers.dropout(inputs, rate=0.5, training=training)\n","\t\t\n","\t\tinputs = Conv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=num_classes,\n","\t\t\t\t\tkernel_size=1,\n","\t\t\t\t\tstrides=1,\n","\t\t\t\t\tuse_bias=True)\n","\n","\t\treturn tf.identity(inputs, 'output')\n","\n","\n","def _encoding_block_layer( inputs, filters, block_fn,\n","\t\t\t\t\t\t\t\tblocks, strides, training, name):\n","\t\t\"\"\"Creates one layer of encoding blocks for the model.\n","\t\tArgs:\n","\t\t\tinputs: A tensor of size [batch, depth_in, height_in, width_in, channels].\n","\t\t\tfilters: The number of filters for the first convolution of the layer.\n","\t\t\tblock_fn: The block to use within the model.\n","\t\t\tblocks: The number of blocks contained in the layer.\n","\t\t\tstrides: The stride to use for the first convolution of the layer. If\n","\t\t\t\tgreater than 1, this layer will ultimately downsample the input.\n","\t\t\ttraining: Either True or False, whether we are currently training the\n","\t\t\t\tmodel. Needed for batch norm.\n","\t\t\tname: A string name for the tensor output of the block layer.\n","\t\tReturns:\n","\t\t\tThe output tensor of the block layer.\n","\t\t\"\"\"\n","\n","\t\tdef projection_shortcut(inputs):\n","\t\t\treturn Conv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=filters,\n","\t\t\t\t\tkernel_size=1,\n","\t\t\t\t\tstrides=strides)\n","\n","\t\t# Only the first block per block_layer uses projection_shortcut and strides\n","\t\tinputs = block_fn(inputs, filters, training, projection_shortcut, strides)\n","\n","\t\tfor _ in range(1, blocks):\n","\t\t\tinputs = block_fn(inputs, filters, training, None, 1)\n","\n","\t\treturn tf.identity(inputs, name)\n","\n","\n","def _att_decoding_block_layer( inputs, skip_inputs, filters,\n","\t\t\t\t\t\t\t\tblock_fn, blocks, strides, training, name):\n","\t\t\"\"\"Creates one layer of decoding blocks for the model.\n","\t\tArgs:\n","\t\t\tinputs: A tensor of size [batch, depth_in, height_in, width_in, channels].\n","\t\t\tskip_inputs: A tensor of size [batch, depth_in, height_in, width_in, filters].\n","\t\t\tfilters: The number of filters for the first convolution of the layer.\n","\t\t\tblock_fn: The block to use within the model.\n","\t\t\tblocks: The number of blocks contained in the layer.\n","\t\t\tstrides: The stride to use for the first convolution of the layer. If\n","\t\t\t\tgreater than 1, this layer will ultimately downsample the input.\n","\t\t\ttraining: Either True or False, whether we are currently training the\n","\t\t\t\tmodel. Needed for batch norm.\n","\t\t\tname: A string name for the tensor output of the block layer.\n","\t\tReturns:\n","\t\t\tThe output tensor of the block layer.\n","\t\t\"\"\"\n","\n","\t\tdef projection_shortcut(inputs):\n","\t\t\treturn Deconv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=filters,\n","\t\t\t\t\tkernel_size=3,\n","\t\t\t\t\tstrides=strides)\n","\n","\t\tinputs = _attention_block(\n","\t\t\t\t\tinputs, filters, training, projection_shortcut, strides)\n","\n","\t\tinputs = inputs + skip_inputs\n","\n","\t\tfor _ in range(0, blocks):\n","\t\t\tinputs = block_fn(inputs, filters, training, None, 1)\n","\n","\t\treturn tf.identity(inputs, name)\n","\n","\n","def _decoding_block_layer( inputs, skip_inputs, filters,\n","\t\t\t\t\t\t\t\tblock_fn, blocks, strides, training, name):\n","\t\t\"\"\"Creates one layer of decoding blocks for the model.\n","\t\tArgs:\n","\t\t\tinputs: A tensor of size [batch, depth_in, height_in, width_in, channels].\n","\t\t\tskip_inputs: A tensor of size [batch, depth_in, height_in, width_in, filters].\n","\t\t\tfilters: The number of filters for the first convolution of the layer.\n","\t\t\tblock_fn: The block to use within the model.\n","\t\t\tblocks: The number of blocks contained in the layer.\n","\t\t\tstrides: The stride to use for the first convolution of the layer. If\n","\t\t\t\tgreater than 1, this layer will ultimately downsample the input.\n","\t\t\ttraining: Either True or False, whether we are currently training the\n","\t\t\t\tmodel. Needed for batch norm.\n","\t\t\tname: A string name for the tensor output of the block layer.\n","\t\tReturns:\n","\t\t\tThe output tensor of the block layer.\n","\t\t\"\"\"\n","\n","\t\tinputs = Deconv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=filters,\n","\t\t\t\t\tkernel_size=3,\n","\t\t\t\t\tstrides=strides)\n","\n","\t\tinputs = inputs + skip_inputs\n","\n","\t\tfor _ in range(0, blocks):\n","\t\t\tinputs = block_fn(inputs, filters, training, None, 1)\n","\n","\t\treturn tf.identity(inputs, name)\n","\n","\n","\t################################################################################\n","\t# Basic blocks building the network\n","\t################################################################################\n","def _residual_block( inputs, filters, training,\n","\t\t\t\t\t\t\tprojection_shortcut, strides):\n","\t\t\"\"\"Standard building block for residual networks with BN before convolutions.\n","\t\tArgs:\n","\t\t\tinputs: A tensor of size [batch, depth_in, height_in, width_in, channels].\n","\t\t\tfilters: The number of filters for the convolutions.\n","\t\t\ttraining: A Boolean for whether the model is in training or inference\n","\t\t\t\tmode. Needed for batch normalization.\n","\t\t\tprojection_shortcut: The function to use for projection shortcuts\n","\t\t\t\t(typically a 1x1 convolution when downsampling the input).\n","\t\t\tstrides: The block's stride. If greater than 1, this block will ultimately\n","\t\t\t\tdownsample the input.\n","\t\tReturns:\n","\t\t\tThe output tensor of the block.\n","\t\t\"\"\"\n","\n","\t\tshortcut = inputs\n","\t\tinputs = BN_ReLU(inputs, training)\n","\n","\t\t# The projection shortcut should come after the first batch norm and ReLU\n","\t\t# since it performs a 1x1 convolution.\n","\t\tif projection_shortcut is not None:\n","\t\t\tshortcut = projection_shortcut(inputs)\n","\n","\t\tinputs = Conv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=filters,\n","\t\t\t\t\tkernel_size=3,\n","\t\t\t\t\tstrides=strides)\n","\n","\t\tinputs = BN_ReLU(inputs, training)\n","\n","\t\tinputs = Conv3D(\n","\t\t\t\t\tinputs=inputs,\n","\t\t\t\t\tfilters=filters,\n","\t\t\t\t\tkernel_size=3,\n","\t\t\t\t\tstrides=1)\n","\n","\t\treturn inputs + shortcut\n","\n","\n","def _attention_block( inputs, filters, training,\n","\t\t\t\t\t\t\tprojection_shortcut, strides):\n","\t\t\"\"\"Attentional building block for residual networks with BN before convolutions.\n","\t\tArgs:\n","\t\t\tinputs: A tensor of size [batch, depth_in, height_in, width_in, channels].\n","\t\t\tfilters: The number of filters for the convolutions.\n","\t\t\ttraining: A Boolean for whether the model is in training or inference\n","\t\t\t\tmode. Needed for batch normalization.\n","\t\t\tprojection_shortcut: The function to use for projection shortcuts\n","\t\t\t\t(typically a 1x1 convolution when downsampling the input).\n","\t\t\tstrides: The block's stride. If greater than 1, this block will ultimately\n","\t\t\t\tdownsample the input.\n","\t\tReturns:\n","\t\t\tThe output tensor of the block.\n","\t\t\"\"\"\n","\n","\t\tshortcut = inputs\n","\t\tinputs = BN_ReLU(inputs, training)\n","\n","\t\t# The projection shortcut should come after the first batch norm and ReLU\n","\t\t# since it performs a 1x1 convolution.\n","\t\tif projection_shortcut is not None:\n","\t\t\tshortcut = projection_shortcut(inputs)\n","\n","\t\tif strides != 1:\n","\t\t\tlayer_type = 'UP'\n","\t\telse:\n","\t\t\tlayer_type = 'SAME'\n","\n","\t\tinputs = multihead_attention_3d(\n","\t\t\t\t\tinputs, filters, filters, filters, 1, training, layer_type)\n","\n","\t\treturn inputs + shortcut"]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n","import numpy as np\n","import cv2\n","from glob import glob\n","from sklearn.utils import shuffle\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import Recall, Precision, Accuracy, IoU\n","from sklearn.model_selection import train_test_split\n","\n","H = 256\n","W = 256"],"metadata":{"id":"FS1wNETdS32I","executionInfo":{"status":"ok","timestamp":1651674321753,"user_tz":-60,"elapsed":6,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","drive_path = '/content/drive/MyDrive/PFA' "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YiSBM-gTJVW","executionInfo":{"status":"ok","timestamp":1651674323946,"user_tz":-60,"elapsed":2198,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}},"outputId":"bdafb504-6f81-4f32-f154-55d59a7cadf0"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def create_dir(path): #TO BE CHANGED\n","    \"\"\"Create a directory\"\"\"\n","    if not os.path.exists(path):\n","        os.mkdir(path)"],"metadata":{"id":"Pe0hMSLLTPbz","executionInfo":{"status":"ok","timestamp":1651674323948,"user_tz":-60,"elapsed":23,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["def load_data(path, split=0.2):\n","\n","    images = sorted(glob(f\"{path}/ROI/*.bmp\"))\n","    masks = sorted(glob(f\"{path}/label/*.bmp\"))\n","    \n","    print(len(masks), len(images))\n","\n","    train_x, valid_x = train_test_split(images, test_size=0.2, random_state=42)\n","    train_y, valid_y = train_test_split(masks, test_size=0.2, random_state=42)\n","    print(len(train_x), len(train_y), len(valid_x), len(valid_y))\n","   \n","    return (train_x, train_y), (valid_x, valid_y)"],"metadata":{"id":"kgmq999OTRCA","executionInfo":{"status":"ok","timestamp":1651674323949,"user_tz":-60,"elapsed":22,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["def shuffling(x, y):\n","    x, y = shuffle(x, y, random_state=42)\n","    return x, y"],"metadata":{"id":"NXhf_UD0TSiH","executionInfo":{"status":"ok","timestamp":1651674323950,"user_tz":-60,"elapsed":21,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["def read_image(path):\n","    path = path.decode()\n","    x = cv2.imread(path, cv2.IMREAD_COLOR)\n","    x = x / 255.0\n","    x = x.astype(np.float32)\n","    return x"],"metadata":{"id":"N-6-HShWTUqU","executionInfo":{"status":"ok","timestamp":1651674323952,"user_tz":-60,"elapsed":22,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["def read_mask(path):\n","    path = path.decode('utf-8')\n","    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","    x[x == 255] = 0\n","    x[x == 223] = 1 #ZP\n","    x[x == 127] = 2 #TE\n","    x[x == 191] = 3 #ICM\n","    x[ x > 3] = 0\n","    x = x.astype(np.int32)\n","    return x"],"metadata":{"id":"cs-4eEVyTVtv","executionInfo":{"status":"ok","timestamp":1651674323953,"user_tz":-60,"elapsed":23,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["def tf_parse(x, y):\n","    def _parse(x, y):\n","        x = read_image(x)\n","        y = read_mask(y)\n","        return x, y\n","\n","    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.int32])\n","    y = tf.one_hot(y, 4, dtype=tf.int32)\n","    x.set_shape([H, W, 3])\n","    y.set_shape([H, W, 4])\n","\n","    return x, y"],"metadata":{"id":"AYCvkszgTW6c","executionInfo":{"status":"ok","timestamp":1651674323955,"user_tz":-60,"elapsed":24,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["def tf_dataset(x, y, batch=8):\n","    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n","    dataset = dataset.shuffle(buffer_size=5000)\n","    dataset = dataset.map(tf_parse)\n","    dataset = dataset.batch(batch)\n","    dataset = dataset.repeat()\n","    dataset = dataset.prefetch(2)\n","\n","    return dataset"],"metadata":{"id":"_dxliGMNTXm_","executionInfo":{"status":"ok","timestamp":1651674323957,"user_tz":-60,"elapsed":26,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["def iou(y_true, y_pred):\n","    def f(y_true, y_pred):\n","        intersection = (y_true * y_pred).sum()\n","        union = y_true.sum() + y_pred.sum() - intersection\n","        x = (intersection + 1e-15) / (union + 1e-15)\n","        x = x.astype(np.float32)\n","        return x\n","    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n","\n","smooth = 1e-15\n","def dice_coef(y_true, y_pred):\n","    y_true = tf.keras.layers.Flatten()(y_true)\n","    y_pred = tf.keras.layers.Flatten()(y_pred)\n","    intersection = tf.reduce_sum(y_true * y_pred)\n","    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n","\n","def dice_loss(y_true, y_pred):\n","    return 1.0 - dice_coef(y_true, y_pred)"],"metadata":{"id":"X0ggDZPHTY7o","executionInfo":{"status":"ok","timestamp":1651674323958,"user_tz":-60,"elapsed":26,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["\"\"\" Seeding \"\"\"\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","\"\"\"Directory for storing files\"\"\"\n","create_dir(\"/content/drive/MyDrive/PFA/files_non_local_unet\")\n","\n","\"\"\"Hyperparameters\"\"\"\n","batch_size = 10\n","learning_rate = 1e-4\n","num_epochs = 30\n","model_path = os.path.join(\"/content/drive/MyDrive/PFA/files_non_local_unet\", \"model.h5\")\n","csv_path = os.path.join(\"/content/drive/MyDrive/PFA/files_non_local_unet\", \"data.csv\")\n","\n","\"\"\"Dataset\"\"\"\n","dataset_path = os.path.join(\"/content/drive/MyDrive/PFA/new_data\")\n","train_path = os.path.join(dataset_path, \"train\")\n","\n","(train_x, train_y), (valid_x, valid_y) = load_data(train_path)\n","\n","\n","print(f\"Train: {len(train_x)} - {len(train_y)}\")\n","print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n","\n","train_dataset = tf_dataset(train_x, train_y, batch=batch_size)\n","\n","test_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\n","\n","\"\"\"Model\"\"\"\n","\n","model = __call__(train_dataset,False)\n","'''\n","metrics = [dice_coef, iou, 'accuracy', Recall(), Precision()]\n","model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate), metrics=metrics)\n","\n","train_steps = len(train_x)//batch_size\n","valid_steps = len(valid_x)//batch_size\n","\n","callbacks = [\n","  ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n","  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-7, verbose=1),\n","  CSVLogger(csv_path),\n","  TensorBoard(),\n","  EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False)\n","]\n","\n","history = model.fit(\n","        train_dataset,\n","        steps_per_epoch=train_steps,\n","        epochs=num_epochs,\n","        validation_data=test_dataset,\n","        validation_steps=valid_steps,\n","        callbacks=callbacks\n","    )\n","  '''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"id":"VcNCbbIWTbet","executionInfo":{"status":"error","timestamp":1651674976242,"user_tz":-60,"elapsed":1264,"user":{"displayName":"HANA BEN ASKER","userId":"03041994240412001663"}},"outputId":"eb9ec3d0-b30b-4dc1-9b1a-f1888b5fb712"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["945 945\n","756 756 189 189\n","Train: 756 - 756\n","Valid: 189 - 189\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: `tf.layers.conv3d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv3D` instead.\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:858: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs)\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-0e8fd3f8e24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\"\"\"Model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m '''\n\u001b[1;32m     33\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdice_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-5bd0dc09b9ff>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(inputs, training)\u001b[0m\n\u001b[1;32m     24\u001b[0m \t\t\"\"\"\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_build_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-5bd0dc09b9ff>\u001b[0m in \u001b[0;36m_build_network\u001b[0;34m(inputs, training)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                         \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \t\t\t\t\tstrides=1)\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'initial_conv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-cd6fa2893285>\u001b[0m in \u001b[0;36mConv3D\u001b[0;34m(inputs, filters, kernel_size, strides, use_bias)\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \t\t\tuse_bias=use_bias)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py\u001b[0m in \u001b[0;36mconv3d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       _scope=name)\n\u001b[0;32m--> 858\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2294\u001b[0m         \u001b[0;34m'Please use `layer.__call__` method instead.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2295\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2296\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_doc_inheritable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inputs to a layer should be tensors. Got: {x}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: <PrefetchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 256, 256, 4), dtype=tf.int32, name=None))>"]}]}]}